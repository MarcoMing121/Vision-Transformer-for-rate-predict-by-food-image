{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import Normalize, Resize, ToTensor, Compose\n",
    "# For dislaying images\n",
    "from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "# Loading dataset\n",
    "from datasets import load_dataset\n",
    "# Transformers\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "# Matrix operations\n",
    "import numpy as np\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torchvision.datasets import DatasetFolder,ImageFolder\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    import platform\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # \"mps\" if platform.system() == 'Darwin' and torch.backends.mps.is_built() \\\n",
    "    #         else \n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_path = 'photo'\n",
    "if os.path.isdir(data_path): print(\"%s exists\" %data_path)\n",
    "else: \n",
    "    raise FileNotFoundError(\"%s directory does not exist. Please download the data.\" % data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper(object):\n",
    "    def __init__(self, num_trials):\n",
    "        self.num_trials = num_trials\n",
    "        self.trial_counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def is_continuable(self, model, loss):\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            self.trial_counter = 0\n",
    "            return True\n",
    "        elif self.trial_counter + 1 < self.num_trials:\n",
    "            self.trial_counter += 1\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Checking -- skip when ran once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove 1 image\\photos\\-BIybLxzoFt2d2zbYRcfHA.jpg  **\n",
      "Remove 1 image\\photos\\-NGY_19QK2zq913HdiYc5A.jpg  **\n",
      "Remove 1 image\\photos\\-YAvSvGUs2ugiJUvIRO6Jw.jpg  **\n",
      "Remove 1 image\\photos\\-ZkmgGLJ7AJTjy96nocMNw.jpg  **\n",
      "Remove 1 image\\photos\\0fac-NlXqfBO2pWRkmM9aw.jpg  **\n",
      "Remove 1 image\\photos\\0TpeNZPs3Gu8s30KVXudcg.jpg  **\n",
      "Remove 1 image\\photos\\1MOGQBWogR8oJr1WgERi9g.jpg  **\n",
      "Remove 1 image\\photos\\1wd_eyhMrTqUmicDmn4_Kw.jpg  **\n",
      "Remove 1 image\\photos\\2S78q98b_VpBD7vkrDE5-A.jpg  **\n",
      "Remove 1 image\\photos\\43fHlHSYQ_79OBJW1aVUxA.jpg  **\n",
      "Remove 1 image\\photos\\5q-sAvIPl0yNeuAbNBPM1g.jpg  **\n",
      "Remove 1 image\\photos\\6bKuH4FOdaaPInF9NmlQHQ.jpg  **\n",
      "Remove 1 image\\photos\\74upe0h6XxwgzqpdnAh_7Q.jpg  **\n",
      "Remove 1 image\\photos\\7xcWPjcE4mxoQ1AjvvKJZg.jpg  **\n",
      "Remove 1 image\\photos\\9BvYOtforBBP6MvvDogtmw.jpg  **\n",
      "Remove 1 image\\photos\\9jBH61ndIcsheo6FtIHArA.jpg  **\n",
      "Remove 1 image\\photos\\9RDbbAZB0HnL4hndCWB58w.jpg  **\n",
      "Remove 1 image\\photos\\9X4YPM8nYFjf7hY8xUdc6Q.jpg  **\n",
      "Remove 1 image\\photos\\AkiGRjaMKHdJyV7bdHsQjw.jpg  **\n",
      "Remove 1 image\\photos\\amM65inTV6wvx0NNZN5qhg.jpg  **\n",
      "Remove 1 image\\photos\\AMSyCOP3-Eb_ivNA8w1Vhw.jpg  **\n",
      "Remove 1 image\\photos\\ARwqGQZaT0p-XpYYjMXgQg.jpg  **\n",
      "Remove 1 image\\photos\\aUDiJhcFKt0exhyj4Q23Ow.jpg  **\n",
      "Remove 1 image\\photos\\B7xR9CuhRpP52PoehQHVow.jpg  **\n",
      "Remove 1 image\\photos\\bf3ymV0YgP7B6rEoriaU2w.jpg  **\n",
      "Remove 1 image\\photos\\C6n0nKVbgLbYmxSiQ_bFsg.jpg  **\n",
      "Remove 1 image\\photos\\c73YwNh1JsYR5Hz-u_bOrg.jpg  **\n",
      "Remove 1 image\\photos\\CA9z96gGA4y9QOes2Y9eGw.jpg  **\n",
      "Remove 1 image\\photos\\CBxmBYD_5CXIL_F-2PDqmA.jpg  **\n",
      "Remove 1 image\\photos\\cNkUV0sInfh_Py5PP8SHtQ.jpg  **\n",
      "Remove 1 image\\photos\\cwwoZcpqdu2MwdDusNyTdg.jpg  **\n",
      "Remove 1 image\\photos\\DB7BlUpO4LAmC1lCN62hqg.jpg  **\n",
      "Remove 1 image\\photos\\DMCTwC3UT2w5QzHOQoqBPw.jpg  **\n",
      "Remove 1 image\\photos\\E7Wpzn-1fCnVJ8_zKpecPQ.jpg  **\n",
      "Remove 1 image\\photos\\feUGw0P5byOq4U40C77tyQ.jpg  **\n",
      "Remove 1 image\\photos\\gJH0d6Sut4eZDlbV0GCByg.jpg  **\n",
      "Remove 1 image\\photos\\GPMWGVjuCsa6fadnZsEplw.jpg  **\n",
      "Remove 1 image\\photos\\GWLmPwKeBnh2b_7Kv_LQ7w.jpg  **\n",
      "Remove 1 image\\photos\\hChXG-gGWxzGvalse3EYmw.jpg  **\n",
      "Remove 1 image\\photos\\hclqCX1FWcV_TtJJoI3BpQ.jpg  **\n",
      "Remove 1 image\\photos\\hjEfal2a1DWRDu8_AUDLNg.jpg  **\n",
      "Remove 1 image\\photos\\IB2ZjqjtS1W_DadQoPPdgg.jpg  **\n",
      "Remove 1 image\\photos\\IExxMfr1h0bxw54jsanyKA.jpg  **\n",
      "Remove 1 image\\photos\\IkGbGxI8IoOCuVsNB0VLrA.jpg  **\n",
      "Remove 1 image\\photos\\IUsKp87a-v9Yhx6Ftg1m5A.jpg  **\n",
      "Remove 1 image\\photos\\iX-8Xm2G7meRHUg8qhoL1A.jpg  **\n",
      "Remove 1 image\\photos\\j5-4lzg23yGECBa6l1fyRQ.jpg  **\n",
      "Remove 1 image\\photos\\JG5s_bvRF1cSWf1fk9lTbw.jpg  **\n",
      "Remove 1 image\\photos\\JGpfPj8VEvnq1B-Xqr3w-A.jpg  **\n",
      "Remove 1 image\\photos\\JoQ5xekjQUkj8rukJIzqgg.jpg  **\n",
      "Remove 1 image\\photos\\jU-dKl2Ye4L_5x602yoctQ.jpg  **\n",
      "Remove 1 image\\photos\\juDNZOOnkgG3QINFrulsAg.jpg  **\n",
      "Remove 1 image\\photos\\JZZ716oX6_MqH6L_MkWK-A.jpg  **\n",
      "Remove 1 image\\photos\\K6pfRNwGodm1m1gFVQlj-Q.jpg  **\n",
      "Remove 1 image\\photos\\ke4ohxa93GJz0KH9H2kwsQ.jpg  **\n",
      "Remove 1 image\\photos\\kjMBhxBXOUE7SSUQb-YQbw.jpg  **\n",
      "Remove 1 image\\photos\\l2vR3PyVMF3pgIERdDEuiQ.jpg  **\n",
      "Remove 1 image\\photos\\LhLfsQtYwJ5OmEzilubhXQ.jpg  **\n",
      "Remove 1 image\\photos\\lrfy4UVIWtj0xwboLgUreQ.jpg  **\n",
      "Remove 1 image\\photos\\LXT4hCf1lRyUeM4HDBaSvg.jpg  **\n",
      "Remove 1 image\\photos\\l_rMdwgrvjm2PyHyXBcBTw.jpg  **\n",
      "Remove 1 image\\photos\\m3oIKhKKCQD54y1E-dBKSw.jpg  **\n",
      "Remove 1 image\\photos\\MduVueqYTBlEkX-axrh1ug.jpg  **\n",
      "Remove 1 image\\photos\\MZj64XNUN6Og178-6XYR6g.jpg  **\n",
      "Remove 1 image\\photos\\N6hL8FQ84A2DznF2S2Lp7g.jpg  **\n",
      "Remove 1 image\\photos\\n6Q9vNuxz7786ESEfautxQ.jpg  **\n",
      "Remove 1 image\\photos\\NfayhoTudVJQsEF-XlPyjw.jpg  **\n",
      "Remove 1 image\\photos\\NKEFWvRriK-LvagPz2QRxw.jpg  **\n",
      "Remove 1 image\\photos\\nKJ7yiPc0E_DJNtNxmCrhg.jpg  **\n",
      "Remove 1 image\\photos\\O0bVFyP58TOEix6IjERXQA.jpg  **\n",
      "Remove 1 image\\photos\\OK6HsALzFcBAUlrroKHZGg.jpg  **\n",
      "Remove 1 image\\photos\\PFD3ykdI1WVhvZ8IX4PmLQ.jpg  **\n",
      "Remove 1 image\\photos\\PjfJoBrEFgDrxiJy8nyatA.jpg  **\n",
      "Remove 1 image\\photos\\Pk87_8Yndygr4LRUD_H7Hg.jpg  **\n",
      "Remove 1 image\\photos\\pW1IPuTdLIUB61goirbXaA.jpg  **\n",
      "Remove 1 image\\photos\\pY32hIagdxrL4Nsi959EQg.jpg  **\n",
      "Remove 1 image\\photos\\QhATx1B1n8uf8C6siMNTfA.jpg  **\n",
      "Remove 1 image\\photos\\qMlGILrsrzhMDxajNYiyIA.jpg  **\n",
      "Remove 1 image\\photos\\QRUo4vqUu3X9V4eIqBpY8A.jpg  **\n",
      "Remove 1 image\\photos\\qxSXsYMA3aWuAfigeqeOOQ.jpg  **\n",
      "Remove 1 image\\photos\\RhC7TNmFvbR9GWrlrl5dsA.jpg  **\n",
      "Remove 1 image\\photos\\RIeulJUzgemFugkkgg4qgA.jpg  **\n",
      "Remove 1 image\\photos\\rIhUkEmP-j4NcQVW3YuPYQ.jpg  **\n",
      "Remove 1 image\\photos\\rLafN9k3_AF5lZU0cs3LZg.jpg  **\n",
      "Remove 1 image\\photos\\RLtBKD2rlfTaELWejmLBCA.jpg  **\n",
      "Remove 1 image\\photos\\rrfwGSwt3eHxxypfu5PGTA.jpg  **\n",
      "Remove 1 image\\photos\\tlp6LCLDsvL1GjO_kW_plQ.jpg  **\n",
      "Remove 1 image\\photos\\TN4-gAea6ejAdZ-NzYXxng.jpg  **\n",
      "Remove 1 image\\photos\\tSHz7RzlgceAItRejZ396A.jpg  **\n",
      "Remove 1 image\\photos\\TvD36_DdnyCJuXV1SSt3_Q.jpg  **\n",
      "Remove 1 image\\photos\\t_sV6mI4oNvbvohhZAyeuA.jpg  **\n",
      "Remove 1 image\\photos\\UG2JuFFa_WxhPEtMOtq-JQ.jpg  **\n",
      "Remove 1 image\\photos\\VSekUmmsGZcX7KaPe_hXyw.jpg  **\n",
      "Remove 1 image\\photos\\w5ABnSadHC8z1lthMQBaBQ.jpg  **\n",
      "Remove 1 image\\photos\\W94rrCn0O5K1lkfD26m4tw.jpg  **\n",
      "Remove 1 image\\photos\\WGmGujPl5BmR_fCUZnoe9w.jpg  **\n",
      "Remove 1 image\\photos\\XX6ujA9CcB5s9y9wCy67-Q.jpg  **\n",
      "Remove 1 image\\photos\\Y3lA41pnMkQNGfyREkf6SA.jpg  **\n",
      "Remove 1 image\\photos\\yAf6R6OSgPo8-mmdDh8qIw.jpg  **\n",
      "Remove 1 image\\photos\\ydm3g1wUWSxJnMPgHk2JhQ.jpg  **\n",
      "Remove 1 image\\photos\\yFjqHyOaNFwzIWTV8EE9hg.jpg  **\n",
      "Remove 1 image\\photos\\yhztPWh5IhaePpUQJNW-dQ.jpg  **\n",
      "Remove 1 image\\photos\\ytJ4lihJrvyzMMRG-WwDNw.jpg  **\n",
      "Remove 1 image\\photos\\YW1WMOkVbdFBrixDnKgoqA.jpg  **\n",
      "Remove 1 image\\photos\\zTzdu2QqLozHpW_qYWF84w.jpg  **\n",
      "Remove 1 image\\photos\\_exWW0g4Svg1Eo2YWsGzbg.jpg  **\n",
      "** Path: photo\\photos\\__ZciZU6rbzL2KNV7rzD3g.jpg  **\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "folder_path = r'photo'\n",
    "extensions = []\n",
    "remove = []\n",
    "for fldr in os.listdir(folder_path):\n",
    "    sub_folder_path = os.path.join(folder_path, fldr)\n",
    "    for file in os.listdir(sub_folder_path):\n",
    "        file_path = os.path.join(sub_folder_path, file)\n",
    "        print('** Path: {}  **'.format(file_path), end=\"\\r\", flush=True)\n",
    "        try:\n",
    "            im = Image.open(file_path)\n",
    "        except:\n",
    "            print('Remove 1 image')\n",
    "            remove.append(file.split('.')[0])\n",
    "        rgb_im = im.convert('RGB')\n",
    "        if file.split('.')[1] not in extensions:\n",
    "            extensions.append(file.split('.')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "remove = ['-BIybLxzoFt2d2zbYRcfHA',\n",
    " '-NGY_19QK2zq913HdiYc5A',\n",
    " '-YAvSvGUs2ugiJUvIRO6Jw',\n",
    " '-ZkmgGLJ7AJTjy96nocMNw',\n",
    " '0fac-NlXqfBO2pWRkmM9aw',\n",
    " '0TpeNZPs3Gu8s30KVXudcg',\n",
    " '1MOGQBWogR8oJr1WgERi9g',\n",
    " '1wd_eyhMrTqUmicDmn4_Kw',\n",
    " '2S78q98b_VpBD7vkrDE5-A',\n",
    " '43fHlHSYQ_79OBJW1aVUxA',\n",
    " '5q-sAvIPl0yNeuAbNBPM1g',\n",
    " '6bKuH4FOdaaPInF9NmlQHQ',\n",
    " '74upe0h6XxwgzqpdnAh_7Q',\n",
    " '7xcWPjcE4mxoQ1AjvvKJZg',\n",
    " '9BvYOtforBBP6MvvDogtmw',\n",
    " '9jBH61ndIcsheo6FtIHArA',\n",
    " '9RDbbAZB0HnL4hndCWB58w',\n",
    " '9X4YPM8nYFjf7hY8xUdc6Q',\n",
    " 'AkiGRjaMKHdJyV7bdHsQjw',\n",
    " 'amM65inTV6wvx0NNZN5qhg',\n",
    " 'AMSyCOP3-Eb_ivNA8w1Vhw',\n",
    " 'ARwqGQZaT0p-XpYYjMXgQg',\n",
    " 'aUDiJhcFKt0exhyj4Q23Ow',\n",
    " 'B7xR9CuhRpP52PoehQHVow',\n",
    " 'bf3ymV0YgP7B6rEoriaU2w',\n",
    " 'C6n0nKVbgLbYmxSiQ_bFsg',\n",
    " 'c73YwNh1JsYR5Hz-u_bOrg',\n",
    " 'CA9z96gGA4y9QOes2Y9eGw',\n",
    " 'CBxmBYD_5CXIL_F-2PDqmA',\n",
    " 'cNkUV0sInfh_Py5PP8SHtQ',\n",
    " 'cwwoZcpqdu2MwdDusNyTdg',\n",
    " 'DB7BlUpO4LAmC1lCN62hqg',\n",
    " 'DMCTwC3UT2w5QzHOQoqBPw',\n",
    " 'E7Wpzn-1fCnVJ8_zKpecPQ',\n",
    " 'feUGw0P5byOq4U40C77tyQ',\n",
    " 'gJH0d6Sut4eZDlbV0GCByg',\n",
    " 'GPMWGVjuCsa6fadnZsEplw',\n",
    " 'GWLmPwKeBnh2b_7Kv_LQ7w',\n",
    " 'hChXG-gGWxzGvalse3EYmw',\n",
    " 'hclqCX1FWcV_TtJJoI3BpQ',\n",
    " 'hjEfal2a1DWRDu8_AUDLNg',\n",
    " 'IB2ZjqjtS1W_DadQoPPdgg',\n",
    " 'IExxMfr1h0bxw54jsanyKA',\n",
    " 'IkGbGxI8IoOCuVsNB0VLrA',\n",
    " 'IUsKp87a-v9Yhx6Ftg1m5A',\n",
    " 'iX-8Xm2G7meRHUg8qhoL1A',\n",
    " 'j5-4lzg23yGECBa6l1fyRQ',\n",
    " 'JG5s_bvRF1cSWf1fk9lTbw',\n",
    " 'JGpfPj8VEvnq1B-Xqr3w-A',\n",
    " 'JoQ5xekjQUkj8rukJIzqgg',\n",
    " 'jU-dKl2Ye4L_5x602yoctQ',\n",
    " 'juDNZOOnkgG3QINFrulsAg',\n",
    " 'JZZ716oX6_MqH6L_MkWK-A',\n",
    " 'K6pfRNwGodm1m1gFVQlj-Q',\n",
    " 'ke4ohxa93GJz0KH9H2kwsQ',\n",
    " 'kjMBhxBXOUE7SSUQb-YQbw',\n",
    " 'l2vR3PyVMF3pgIERdDEuiQ',\n",
    " 'LhLfsQtYwJ5OmEzilubhXQ',\n",
    " 'lrfy4UVIWtj0xwboLgUreQ',\n",
    " 'LXT4hCf1lRyUeM4HDBaSvg',\n",
    " 'l_rMdwgrvjm2PyHyXBcBTw',\n",
    " 'm3oIKhKKCQD54y1E-dBKSw',\n",
    " 'MduVueqYTBlEkX-axrh1ug',\n",
    " 'MZj64XNUN6Og178-6XYR6g',\n",
    " 'N6hL8FQ84A2DznF2S2Lp7g',\n",
    " 'n6Q9vNuxz7786ESEfautxQ',\n",
    " 'NfayhoTudVJQsEF-XlPyjw',\n",
    " 'NKEFWvRriK-LvagPz2QRxw',\n",
    " 'nKJ7yiPc0E_DJNtNxmCrhg',\n",
    " 'O0bVFyP58TOEix6IjERXQA',\n",
    " 'OK6HsALzFcBAUlrroKHZGg',\n",
    " 'PFD3ykdI1WVhvZ8IX4PmLQ',\n",
    " 'PjfJoBrEFgDrxiJy8nyatA',\n",
    " 'Pk87_8Yndygr4LRUD_H7Hg',\n",
    " 'pW1IPuTdLIUB61goirbXaA',\n",
    " 'pY32hIagdxrL4Nsi959EQg',\n",
    " 'QhATx1B1n8uf8C6siMNTfA',\n",
    " 'qMlGILrsrzhMDxajNYiyIA',\n",
    " 'QRUo4vqUu3X9V4eIqBpY8A',\n",
    " 'qxSXsYMA3aWuAfigeqeOOQ',\n",
    " 'RhC7TNmFvbR9GWrlrl5dsA',\n",
    " 'RIeulJUzgemFugkkgg4qgA',\n",
    " 'rIhUkEmP-j4NcQVW3YuPYQ',\n",
    " 'rLafN9k3_AF5lZU0cs3LZg',\n",
    " 'RLtBKD2rlfTaELWejmLBCA',\n",
    " 'rrfwGSwt3eHxxypfu5PGTA',\n",
    " 'tlp6LCLDsvL1GjO_kW_plQ',\n",
    " 'TN4-gAea6ejAdZ-NzYXxng',\n",
    " 'tSHz7RzlgceAItRejZ396A',\n",
    " 'TvD36_DdnyCJuXV1SSt3_Q',\n",
    " 't_sV6mI4oNvbvohhZAyeuA',\n",
    " 'UG2JuFFa_WxhPEtMOtq-JQ',\n",
    " 'VSekUmmsGZcX7KaPe_hXyw',\n",
    " 'w5ABnSadHC8z1lthMQBaBQ',\n",
    " 'W94rrCn0O5K1lkfD26m4tw',\n",
    " 'WGmGujPl5BmR_fCUZnoe9w',\n",
    " 'XX6ujA9CcB5s9y9wCy67-Q',\n",
    " 'Y3lA41pnMkQNGfyREkf6SA',\n",
    " 'yAf6R6OSgPo8-mmdDh8qIw',\n",
    " 'ydm3g1wUWSxJnMPgHk2JhQ',\n",
    " 'yFjqHyOaNFwzIWTV8EE9hg',\n",
    " 'yhztPWh5IhaePpUQJNW-dQ',\n",
    " 'ytJ4lihJrvyzMMRG-WwDNw',\n",
    " 'YW1WMOkVbdFBrixDnKgoqA',\n",
    " 'zTzdu2QqLozHpW_qYWF84w',\n",
    " '_exWW0g4Svg1Eo2YWsGzbg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "from tqdm import tqdm\n",
    "def load_business_rate(json_path):\n",
    "    \"\"\"\n",
    "    This function loads the image targets from a csv file. It assumes that the csv file\n",
    "    has a header row and that the first column contains the image path and all the subsequent\n",
    "    columns contain the target values which are bundled together into a numpy array.\n",
    "    \"\"\"\n",
    "    business_dict = {}\n",
    "    with open(json_path,'r') as f:\n",
    "        f = ijson.parse(f,multiple_values = True)\n",
    "        for line in ijson.items(f,\"\"):\n",
    "            key = line['business_id']\n",
    "            value = line['stars']\n",
    "            business_dict.setdefault(key,[]).append(value)\n",
    "\n",
    "    for key in business_dict.keys():\n",
    "        value = business_dict.get(key,None)\n",
    "        business_dict[key] = [round(sum(value)/len(value))]      \n",
    "    return business_dict\n",
    "\n",
    "def load_image_rate(json_path, business_dict):\n",
    "    with open(json_path,'r') as f:\n",
    "        f = ijson.parse(f,multiple_values = True)\n",
    "        for line in ijson.items(f,''):\n",
    "            if line['label'] == 'food':\n",
    "                if line['business_id'] in business_dict.keys() and line['photo_id'] not in remove:\n",
    "                    key = line['business_id']\n",
    "                    business_dict.get(key,None).append(line['photo_id']+'.jpg')\n",
    "    target_dict = {}\n",
    "    for k,v in business_dict.items():\n",
    "        rate = v.pop(0)\n",
    "        for item in v:\n",
    "            target_dict[item] = rate\n",
    "    business_dict.clear()\n",
    "    return target_dict\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RegressionImageFolder(torchvision.datasets.ImageFolder):\n",
    "    \"\"\"\n",
    "    The regression image folder is a subclass of the ImageFolder class and is designed for \n",
    "    image regression tasks rather than image classification tasks. It takes in a dictionary\n",
    "    that maps image paths to their target values.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, image_targets, *args, **kwargs):\n",
    "        super().__init__(root, *args, **kwargs)\n",
    "        paths, _ = zip(*self.imgs)\n",
    "        prefix = paths[0].split('\\\\')[0] + '\\\\'\n",
    "        \n",
    "        # filtered_list = [item for item in my_list if item in my_dict]\n",
    "        self.targets = list(image_targets.values())\n",
    "        paths = [ prefix + k for k in image_targets.keys()]\n",
    "        self.samples = self.imgs = list(zip(paths, self.targets))\n",
    "\n",
    "def make_data(train_tfm):\n",
    "    \"\"\"\n",
    "    Builds the train data loader\n",
    "    \"\"\"\n",
    "    business_dict = load_business_rate('yelp_academic_dataset_review.json')\n",
    "    targets = load_image_rate('photos.json', business_dict)\n",
    "    data = RegressionImageFolder(\n",
    "        'photo/', \n",
    "        image_targets= targets,\n",
    "        loader=lambda x: Image.open(x),\n",
    "        transform = train_tfm\n",
    "    )\n",
    "    # This constructs the dataloader that actually determins how images will be loaded in batches\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/vit-base-patch16-224\"\n",
    "processor = ViTImageProcessor.from_pretrained(model_name) \n",
    "\n",
    "mu, sigma = processor.image_mean, processor.image_std #get default mu,sigma\n",
    "size = processor.size\n",
    "\n",
    "norm = Normalize(mean=mu, std=sigma) #normalize image pixels range to [-1,1]\n",
    "\n",
    "# resize 3x32x32 to 3x224x224 -> convert to Pytorch tensor -> normalize\n",
    "transf = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    norm\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = make_data(transf)\n",
    "\n",
    "\n",
    "len_data = len(data_set)\n",
    "test_split = [int(len_data*0.8),len_data - int(len_data*0.8)]\n",
    "trainset_data, test_data = random_split(dataset=data_set, lengths=test_split,generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "len_train = len(trainset_data)\n",
    "val_split = [int(len_train*0.9),len_train - int(len_train*0.9)]\n",
    "train_data, val_data = random_split(dataset=trainset_data, lengths=val_split,generator=torch.Generator().manual_seed(0))\n",
    "input_dim = train_data[0][0].shape[0]\n",
    "\n",
    "def load_data(batch_size):\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader\n",
    "train_loader, val_loader, test_loader = load_data(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training - Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "itos = dict((i,i+1) for i in range(5))\n",
    "stoi = dict((i+1,i) for i in range(5))\n",
    "model = ViTForImageClassification.from_pretrained(model_name, num_labels=6, ignore_mismatched_sizes=True, id2label=itos, label2id=stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"photo_to_rate\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=1e-2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_dir='logs',\n",
    "    remove_unused_columns=False,\n",
    "    fp16 = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixels = torch.stack([example[0] for example in examples])\n",
    "    labels = torch.tensor([example[1] for example in examples])\n",
    "    return {\"pixel_values\": pixels, \"labels\": labels}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'accuracy': accuracy_score(predictions, labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-BIybLxzoFt2d2zbYRcfHA',\n",
       " '-NGY_19QK2zq913HdiYc5A',\n",
       " '-YAvSvGUs2ugiJUvIRO6Jw',\n",
       " '-ZkmgGLJ7AJTjy96nocMNw',\n",
       " '0fac-NlXqfBO2pWRkmM9aw',\n",
       " '0TpeNZPs3Gu8s30KVXudcg',\n",
       " '1MOGQBWogR8oJr1WgERi9g',\n",
       " '1wd_eyhMrTqUmicDmn4_Kw',\n",
       " '2S78q98b_VpBD7vkrDE5-A',\n",
       " '43fHlHSYQ_79OBJW1aVUxA',\n",
       " '5q-sAvIPl0yNeuAbNBPM1g',\n",
       " '6bKuH4FOdaaPInF9NmlQHQ',\n",
       " '74upe0h6XxwgzqpdnAh_7Q',\n",
       " '7xcWPjcE4mxoQ1AjvvKJZg',\n",
       " '9BvYOtforBBP6MvvDogtmw',\n",
       " '9jBH61ndIcsheo6FtIHArA',\n",
       " '9RDbbAZB0HnL4hndCWB58w',\n",
       " '9X4YPM8nYFjf7hY8xUdc6Q',\n",
       " 'AkiGRjaMKHdJyV7bdHsQjw',\n",
       " 'amM65inTV6wvx0NNZN5qhg',\n",
       " 'AMSyCOP3-Eb_ivNA8w1Vhw',\n",
       " 'ARwqGQZaT0p-XpYYjMXgQg',\n",
       " 'aUDiJhcFKt0exhyj4Q23Ow',\n",
       " 'B7xR9CuhRpP52PoehQHVow',\n",
       " 'bf3ymV0YgP7B6rEoriaU2w',\n",
       " 'C6n0nKVbgLbYmxSiQ_bFsg',\n",
       " 'c73YwNh1JsYR5Hz-u_bOrg',\n",
       " 'CA9z96gGA4y9QOes2Y9eGw',\n",
       " 'CBxmBYD_5CXIL_F-2PDqmA',\n",
       " 'cNkUV0sInfh_Py5PP8SHtQ',\n",
       " 'cwwoZcpqdu2MwdDusNyTdg',\n",
       " 'DB7BlUpO4LAmC1lCN62hqg',\n",
       " 'DMCTwC3UT2w5QzHOQoqBPw',\n",
       " 'E7Wpzn-1fCnVJ8_zKpecPQ',\n",
       " 'feUGw0P5byOq4U40C77tyQ',\n",
       " 'gJH0d6Sut4eZDlbV0GCByg',\n",
       " 'GPMWGVjuCsa6fadnZsEplw',\n",
       " 'GWLmPwKeBnh2b_7Kv_LQ7w',\n",
       " 'hChXG-gGWxzGvalse3EYmw',\n",
       " 'hclqCX1FWcV_TtJJoI3BpQ',\n",
       " 'hjEfal2a1DWRDu8_AUDLNg',\n",
       " 'IB2ZjqjtS1W_DadQoPPdgg',\n",
       " 'IExxMfr1h0bxw54jsanyKA',\n",
       " 'IkGbGxI8IoOCuVsNB0VLrA',\n",
       " 'IUsKp87a-v9Yhx6Ftg1m5A',\n",
       " 'iX-8Xm2G7meRHUg8qhoL1A',\n",
       " 'j5-4lzg23yGECBa6l1fyRQ',\n",
       " 'JG5s_bvRF1cSWf1fk9lTbw',\n",
       " 'JGpfPj8VEvnq1B-Xqr3w-A',\n",
       " 'JoQ5xekjQUkj8rukJIzqgg',\n",
       " 'jU-dKl2Ye4L_5x602yoctQ',\n",
       " 'juDNZOOnkgG3QINFrulsAg',\n",
       " 'JZZ716oX6_MqH6L_MkWK-A',\n",
       " 'K6pfRNwGodm1m1gFVQlj-Q',\n",
       " 'ke4ohxa93GJz0KH9H2kwsQ',\n",
       " 'kjMBhxBXOUE7SSUQb-YQbw',\n",
       " 'l2vR3PyVMF3pgIERdDEuiQ',\n",
       " 'LhLfsQtYwJ5OmEzilubhXQ',\n",
       " 'lrfy4UVIWtj0xwboLgUreQ',\n",
       " 'LXT4hCf1lRyUeM4HDBaSvg',\n",
       " 'l_rMdwgrvjm2PyHyXBcBTw',\n",
       " 'm3oIKhKKCQD54y1E-dBKSw',\n",
       " 'MduVueqYTBlEkX-axrh1ug',\n",
       " 'MZj64XNUN6Og178-6XYR6g',\n",
       " 'N6hL8FQ84A2DznF2S2Lp7g',\n",
       " 'n6Q9vNuxz7786ESEfautxQ',\n",
       " 'NfayhoTudVJQsEF-XlPyjw',\n",
       " 'NKEFWvRriK-LvagPz2QRxw',\n",
       " 'nKJ7yiPc0E_DJNtNxmCrhg',\n",
       " 'O0bVFyP58TOEix6IjERXQA',\n",
       " 'OK6HsALzFcBAUlrroKHZGg',\n",
       " 'PFD3ykdI1WVhvZ8IX4PmLQ',\n",
       " 'PjfJoBrEFgDrxiJy8nyatA',\n",
       " 'Pk87_8Yndygr4LRUD_H7Hg',\n",
       " 'pW1IPuTdLIUB61goirbXaA',\n",
       " 'pY32hIagdxrL4Nsi959EQg',\n",
       " 'QhATx1B1n8uf8C6siMNTfA',\n",
       " 'qMlGILrsrzhMDxajNYiyIA',\n",
       " 'QRUo4vqUu3X9V4eIqBpY8A',\n",
       " 'qxSXsYMA3aWuAfigeqeOOQ',\n",
       " 'RhC7TNmFvbR9GWrlrl5dsA',\n",
       " 'RIeulJUzgemFugkkgg4qgA',\n",
       " 'rIhUkEmP-j4NcQVW3YuPYQ',\n",
       " 'rLafN9k3_AF5lZU0cs3LZg',\n",
       " 'RLtBKD2rlfTaELWejmLBCA',\n",
       " 'rrfwGSwt3eHxxypfu5PGTA',\n",
       " 'tlp6LCLDsvL1GjO_kW_plQ',\n",
       " 'TN4-gAea6ejAdZ-NzYXxng',\n",
       " 'tSHz7RzlgceAItRejZ396A',\n",
       " 'TvD36_DdnyCJuXV1SSt3_Q',\n",
       " 't_sV6mI4oNvbvohhZAyeuA',\n",
       " 'UG2JuFFa_WxhPEtMOtq-JQ',\n",
       " 'VSekUmmsGZcX7KaPe_hXyw',\n",
       " 'w5ABnSadHC8z1lthMQBaBQ',\n",
       " 'W94rrCn0O5K1lkfD26m4tw',\n",
       " 'WGmGujPl5BmR_fCUZnoe9w',\n",
       " 'XX6ujA9CcB5s9y9wCy67-Q',\n",
       " 'Y3lA41pnMkQNGfyREkf6SA',\n",
       " 'yAf6R6OSgPo8-mmdDh8qIw',\n",
       " 'ydm3g1wUWSxJnMPgHk2JhQ',\n",
       " 'yFjqHyOaNFwzIWTV8EE9hg',\n",
       " 'yhztPWh5IhaePpUQJNW-dQ',\n",
       " 'ytJ4lihJrvyzMMRG-WwDNw',\n",
       " 'YW1WMOkVbdFBrixDnKgoqA',\n",
       " 'zTzdu2QqLozHpW_qYWF84w',\n",
       " '_exWW0g4Svg1Eo2YWsGzbg']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marcowong\\anaconda3\\envs\\temp\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa484ddc2b7e40a9a3eb9bd1474d907f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7118, 'grad_norm': 4.9719038009643555, 'learning_rate': 1.9658611825192804e-05, 'epoch': 0.05}\n",
      "{'loss': 0.674, 'grad_norm': 3.05021595954895, 'learning_rate': 1.931585261353899e-05, 'epoch': 0.1}\n",
      "{'loss': 0.6752, 'grad_norm': 4.472777843475342, 'learning_rate': 1.8973093401885177e-05, 'epoch': 0.15}\n",
      "{'loss': 0.6426, 'grad_norm': 4.0106964111328125, 'learning_rate': 1.8630334190231365e-05, 'epoch': 0.21}\n",
      "{'loss': 0.6262, 'grad_norm': 3.5623209476470947, 'learning_rate': 1.8288260497000857e-05, 'epoch': 0.26}\n",
      "{'loss': 0.6435, 'grad_norm': 3.9394114017486572, 'learning_rate': 1.7945501285347045e-05, 'epoch': 0.31}\n",
      "{'loss': 0.6337, 'grad_norm': 5.440028190612793, 'learning_rate': 1.760274207369323e-05, 'epoch': 0.36}\n",
      "{'loss': 0.6273, 'grad_norm': 2.9689371585845947, 'learning_rate': 1.7259982862039418e-05, 'epoch': 0.41}\n",
      "{'loss': 0.6019, 'grad_norm': 3.333549737930298, 'learning_rate': 1.6917223650385606e-05, 'epoch': 0.46}\n",
      "{'loss': 0.6005, 'grad_norm': 10.020845413208008, 'learning_rate': 1.6575149957155098e-05, 'epoch': 0.51}\n",
      "{'loss': 0.622, 'grad_norm': 4.8251800537109375, 'learning_rate': 1.6232390745501286e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6029, 'grad_norm': 9.310948371887207, 'learning_rate': 1.5889631533847474e-05, 'epoch': 0.62}\n",
      "{'loss': 0.6205, 'grad_norm': 5.406684875488281, 'learning_rate': 1.554687232219366e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6272, 'grad_norm': 12.048330307006836, 'learning_rate': 1.5204798628963156e-05, 'epoch': 0.72}\n",
      "{'loss': 0.603, 'grad_norm': 8.190381050109863, 'learning_rate': 1.4862039417309343e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6001, 'grad_norm': 5.224921703338623, 'learning_rate': 1.4519280205655527e-05, 'epoch': 0.82}\n",
      "{'loss': 0.5772, 'grad_norm': 2.800682783126831, 'learning_rate': 1.4176520994001714e-05, 'epoch': 0.87}\n",
      "{'loss': 0.5963, 'grad_norm': 3.880742073059082, 'learning_rate': 1.383444730077121e-05, 'epoch': 0.93}\n",
      "{'loss': 0.5735, 'grad_norm': 3.309702157974243, 'learning_rate': 1.3491688089117396e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7158e30c67f487c952f88123b13dc31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0246981382369995, 'eval_accuracy': 0.6619620546043499, 'eval_runtime': 81.4434, 'eval_samples_per_second': 106.135, 'eval_steps_per_second': 26.534, 'epoch': 1.0}\n",
      "{'loss': 0.4722, 'grad_norm': 8.607462882995605, 'learning_rate': 1.3148928877463582e-05, 'epoch': 1.03}\n",
      "{'loss': 0.3835, 'grad_norm': 3.5362110137939453, 'learning_rate': 1.2806855184233077e-05, 'epoch': 1.08}\n",
      "{'loss': 0.3921, 'grad_norm': 8.042865753173828, 'learning_rate': 1.2464095972579264e-05, 'epoch': 1.13}\n",
      "{'loss': 0.3763, 'grad_norm': 13.1644926071167, 'learning_rate': 1.212133676092545e-05, 'epoch': 1.18}\n",
      "{'loss': 0.4041, 'grad_norm': 9.167098045349121, 'learning_rate': 1.1778577549271637e-05, 'epoch': 1.23}\n",
      "{'loss': 0.4028, 'grad_norm': 13.384492874145508, 'learning_rate': 1.1435818337617823e-05, 'epoch': 1.29}\n",
      "{'loss': 0.3795, 'grad_norm': 12.04263687133789, 'learning_rate': 1.1093059125964011e-05, 'epoch': 1.34}\n",
      "{'loss': 0.4048, 'grad_norm': 6.883047103881836, 'learning_rate': 1.0750299914310198e-05, 'epoch': 1.39}\n",
      "{'loss': 0.3791, 'grad_norm': 7.202192306518555, 'learning_rate': 1.0407540702656384e-05, 'epoch': 1.44}\n",
      "{'loss': 0.3618, 'grad_norm': 2.3019933700561523, 'learning_rate': 1.006546700942588e-05, 'epoch': 1.49}\n",
      "{'loss': 0.3899, 'grad_norm': 14.14448356628418, 'learning_rate': 9.722707797772066e-06, 'epoch': 1.54}\n",
      "{'loss': 0.3709, 'grad_norm': 6.680057048797607, 'learning_rate': 9.379948586118252e-06, 'epoch': 1.59}\n",
      "{'loss': 0.3724, 'grad_norm': 6.014231204986572, 'learning_rate': 9.037189374464439e-06, 'epoch': 1.65}\n",
      "{'loss': 0.355, 'grad_norm': 1.3522673845291138, 'learning_rate': 8.695115681233934e-06, 'epoch': 1.7}\n",
      "{'loss': 0.3653, 'grad_norm': 0.6057084798812866, 'learning_rate': 8.35235646958012e-06, 'epoch': 1.75}\n",
      "{'loss': 0.5145, 'grad_norm': 4.859604835510254, 'learning_rate': 8.009597257926307e-06, 'epoch': 1.8}\n",
      "{'loss': 0.5466, 'grad_norm': 7.026863098144531, 'learning_rate': 7.666838046272495e-06, 'epoch': 1.85}\n",
      "{'loss': 0.5473, 'grad_norm': 5.363871097564697, 'learning_rate': 7.3240788346186805e-06, 'epoch': 1.9}\n",
      "{'loss': 0.5686, 'grad_norm': 1.7914528846740723, 'learning_rate': 6.981319622964867e-06, 'epoch': 1.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654f86e781c0463daec2995b53f6e79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1215083599090576, 'eval_accuracy': 0.6602267468764461, 'eval_runtime': 102.212, 'eval_samples_per_second': 84.569, 'eval_steps_per_second': 21.142, 'epoch': 2.0}\n",
      "{'loss': 0.5317, 'grad_norm': 4.492599010467529, 'learning_rate': 6.638560411311055e-06, 'epoch': 2.01}\n",
      "{'loss': 0.279, 'grad_norm': 2.145479440689087, 'learning_rate': 6.295801199657241e-06, 'epoch': 2.06}\n",
      "{'loss': 0.2684, 'grad_norm': 2.728562116622925, 'learning_rate': 5.953041988003429e-06, 'epoch': 2.11}\n",
      "{'loss': 0.3165, 'grad_norm': 8.011964797973633, 'learning_rate': 5.610282776349615e-06, 'epoch': 2.16}\n",
      "{'loss': 0.2917, 'grad_norm': 0.8533404469490051, 'learning_rate': 5.268209083119109e-06, 'epoch': 2.21}\n",
      "{'loss': 0.3085, 'grad_norm': 3.2253170013427734, 'learning_rate': 4.925449871465296e-06, 'epoch': 2.26}\n",
      "{'loss': 0.2908, 'grad_norm': 1.9137020111083984, 'learning_rate': 4.582690659811483e-06, 'epoch': 2.31}\n",
      "{'loss': 0.2929, 'grad_norm': 9.928186416625977, 'learning_rate': 4.23993144815767e-06, 'epoch': 2.37}\n",
      "{'loss': 0.2888, 'grad_norm': 2.632171630859375, 'learning_rate': 3.897172236503856e-06, 'epoch': 2.42}\n",
      "{'loss': 0.2904, 'grad_norm': 2.4957473278045654, 'learning_rate': 3.554413024850043e-06, 'epoch': 2.47}\n",
      "{'loss': 0.2822, 'grad_norm': 3.921334981918335, 'learning_rate': 3.2116538131962297e-06, 'epoch': 2.52}\n",
      "{'loss': 0.2899, 'grad_norm': 15.609225273132324, 'learning_rate': 2.8695801199657243e-06, 'epoch': 2.57}\n",
      "{'loss': 0.324, 'grad_norm': 1.9607434272766113, 'learning_rate': 2.526820908311911e-06, 'epoch': 2.62}\n",
      "{'loss': 0.2802, 'grad_norm': 15.277162551879883, 'learning_rate': 2.1847472150814057e-06, 'epoch': 2.67}\n",
      "{'loss': 0.2924, 'grad_norm': 8.413471221923828, 'learning_rate': 1.8419880034275923e-06, 'epoch': 2.72}\n",
      "{'loss': 0.3213, 'grad_norm': 11.552440643310547, 'learning_rate': 1.4992287917737791e-06, 'epoch': 2.78}\n",
      "{'loss': 0.2914, 'grad_norm': 7.367829322814941, 'learning_rate': 1.156469580119966e-06, 'epoch': 2.83}\n",
      "{'loss': 0.2844, 'grad_norm': 4.335596084594727, 'learning_rate': 8.137103684661527e-07, 'epoch': 2.88}\n",
      "{'loss': 0.2909, 'grad_norm': 8.965629577636719, 'learning_rate': 4.7095115681233935e-07, 'epoch': 2.93}\n",
      "{'loss': 0.2983, 'grad_norm': 2.8465375900268555, 'learning_rate': 1.2819194515852613e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9210f17f8042df9f51f5b5f26016ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6411774158477783, 'eval_accuracy': 0.6525913928736696, 'eval_runtime': 87.0781, 'eval_samples_per_second': 99.267, 'eval_steps_per_second': 24.817, 'epoch': 3.0}\n",
      "{'train_runtime': 4584.5544, 'train_samples_per_second': 50.905, 'train_steps_per_second': 6.364, 'train_loss': 0.44653482759111235, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=29175, training_loss=0.44653482759111235, metrics={'train_runtime': 4584.5544, 'train_samples_per_second': 50.905, 'train_steps_per_second': 6.364, 'train_loss': 0.44653482759111235, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=processor)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args, \n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b6946d09aa49b59138de8b5771cc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5403 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 1.0347788333892822, 'test_accuracy': 0.6640444238778344, 'test_runtime': 734.6712, 'test_samples_per_second': 29.415, 'test_steps_per_second': 7.354}\n"
     ]
    }
   ],
   "source": [
    "outputs = trainer.predict(test_data)\n",
    "print(outputs.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas to pd\n",
    "y_true = outputs.label_ids\n",
    "y_pred = outputs.predictions.argmax(1)\n",
    "labels = np.array(itos.values())\n",
    "\n",
    "data = {'y_true': y_true, 'y_pred': y_pred}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the dataframe as a CSV file\n",
    "df.to_csv('output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0   150    11     6     0]\n",
      " [    0  1073   371   197    20]\n",
      " [    0   480  1575  2232   134]\n",
      " [    0    87   956 11224   795]\n",
      " [    0     3    73  1745   478]]\n"
     ]
    }
   ],
   "source": [
    "# rating range: 1 to 5\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.89820359 0.06586826 0.03592814 0.        ]\n",
      " [0.         0.64599639 0.22335942 0.11860325 0.01204094]\n",
      " [0.         0.10857272 0.35625424 0.50486315 0.03030988]\n",
      " [0.         0.00666054 0.0731894  0.85928648 0.06086357]\n",
      " [0.         0.00130492 0.03175294 0.75902566 0.20791649]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def normalize_confusion_matrix(confusion_matrix):\n",
    "    row_sum = np.sum(confusion_matrix, axis=1, keepdims=True)\n",
    "    \n",
    "    normalized_matrix = confusion_matrix / row_sum\n",
    "    np.set_printoptions(suppress=True)\n",
    "    return normalized_matrix\n",
    "normalized_by_row = normalize_confusion_matrix(cm)\n",
    "print(normalized_by_row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "temp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
