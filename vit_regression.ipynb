{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import Normalize, Resize, ToTensor, Compose\n",
    "# For dislaying images\n",
    "from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "# Loading dataset\n",
    "from datasets import load_dataset\n",
    "# Transformers\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "# Matrix operations\n",
    "import numpy as np\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torchvision.datasets import DatasetFolder,ImageFolder\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    import platform\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # \"mps\" if platform.system() == 'Darwin' and torch.backends.mps.is_built() \\\n",
    "    #         else \n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_path = 'photo'\n",
    "if os.path.isdir(data_path): print(\"%s exists\" %data_path)\n",
    "else: \n",
    "    raise FileNotFoundError(\"%s directory does not exist. Please download the data.\" % data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper(object):\n",
    "    def __init__(self, num_trials):\n",
    "        self.num_trials = num_trials\n",
    "        self.trial_counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def is_continuable(self, model, loss):\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            self.trial_counter = 0\n",
    "            return True\n",
    "        elif self.trial_counter + 1 < self.num_trials:\n",
    "            self.trial_counter += 1\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Checking -- skip when ran once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove 1 image\\photos\\-BIybLxzoFt2d2zbYRcfHA.jpg  **\n",
      "Remove 1 image\\photos\\-NGY_19QK2zq913HdiYc5A.jpg  **\n",
      "Remove 1 image\\photos\\-YAvSvGUs2ugiJUvIRO6Jw.jpg  **\n",
      "Remove 1 image\\photos\\-ZkmgGLJ7AJTjy96nocMNw.jpg  **\n",
      "Remove 1 image\\photos\\0fac-NlXqfBO2pWRkmM9aw.jpg  **\n",
      "Remove 1 image\\photos\\0TpeNZPs3Gu8s30KVXudcg.jpg  **\n",
      "** Path: photo\\photos\\0wAjchhtkxi3k0-kwr1QJQ.jpg  **\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "folder_path = r'photo'\n",
    "extensions = []\n",
    "remove = []\n",
    "for fldr in os.listdir(folder_path):\n",
    "    sub_folder_path = os.path.join(folder_path, fldr)\n",
    "    for file in os.listdir(sub_folder_path):\n",
    "        file_path = os.path.join(sub_folder_path, file)\n",
    "        print('** Path: {}  **'.format(file_path), end=\"\\r\", flush=True)\n",
    "        try:\n",
    "            im = Image.open(file_path)\n",
    "        except:\n",
    "            print('Remove 1 image')\n",
    "            remove.append(file.split('.')[0])\n",
    "        rgb_im = im.convert('RGB')\n",
    "        if file.split('.')[1] not in extensions:\n",
    "            extensions.append(file.split('.')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "remove = ['-BIybLxzoFt2d2zbYRcfHA',\n",
    " '-NGY_19QK2zq913HdiYc5A',\n",
    " '-YAvSvGUs2ugiJUvIRO6Jw',\n",
    " '-ZkmgGLJ7AJTjy96nocMNw',\n",
    " '0fac-NlXqfBO2pWRkmM9aw',\n",
    " '0TpeNZPs3Gu8s30KVXudcg',\n",
    " '1MOGQBWogR8oJr1WgERi9g',\n",
    " '1wd_eyhMrTqUmicDmn4_Kw',\n",
    " '2S78q98b_VpBD7vkrDE5-A',\n",
    " '43fHlHSYQ_79OBJW1aVUxA',\n",
    " '5q-sAvIPl0yNeuAbNBPM1g',\n",
    " '6bKuH4FOdaaPInF9NmlQHQ',\n",
    " '74upe0h6XxwgzqpdnAh_7Q',\n",
    " '7xcWPjcE4mxoQ1AjvvKJZg',\n",
    " '9BvYOtforBBP6MvvDogtmw',\n",
    " '9jBH61ndIcsheo6FtIHArA',\n",
    " '9RDbbAZB0HnL4hndCWB58w',\n",
    " '9X4YPM8nYFjf7hY8xUdc6Q',\n",
    " 'AkiGRjaMKHdJyV7bdHsQjw',\n",
    " 'amM65inTV6wvx0NNZN5qhg',\n",
    " 'AMSyCOP3-Eb_ivNA8w1Vhw',\n",
    " 'ARwqGQZaT0p-XpYYjMXgQg',\n",
    " 'aUDiJhcFKt0exhyj4Q23Ow',\n",
    " 'B7xR9CuhRpP52PoehQHVow',\n",
    " 'bf3ymV0YgP7B6rEoriaU2w',\n",
    " 'C6n0nKVbgLbYmxSiQ_bFsg',\n",
    " 'c73YwNh1JsYR5Hz-u_bOrg',\n",
    " 'CA9z96gGA4y9QOes2Y9eGw',\n",
    " 'CBxmBYD_5CXIL_F-2PDqmA',\n",
    " 'cNkUV0sInfh_Py5PP8SHtQ',\n",
    " 'cwwoZcpqdu2MwdDusNyTdg',\n",
    " 'DB7BlUpO4LAmC1lCN62hqg',\n",
    " 'DMCTwC3UT2w5QzHOQoqBPw',\n",
    " 'E7Wpzn-1fCnVJ8_zKpecPQ',\n",
    " 'feUGw0P5byOq4U40C77tyQ',\n",
    " 'gJH0d6Sut4eZDlbV0GCByg',\n",
    " 'GPMWGVjuCsa6fadnZsEplw',\n",
    " 'GWLmPwKeBnh2b_7Kv_LQ7w',\n",
    " 'hChXG-gGWxzGvalse3EYmw',\n",
    " 'hclqCX1FWcV_TtJJoI3BpQ',\n",
    " 'hjEfal2a1DWRDu8_AUDLNg',\n",
    " 'IB2ZjqjtS1W_DadQoPPdgg',\n",
    " 'IExxMfr1h0bxw54jsanyKA',\n",
    " 'IkGbGxI8IoOCuVsNB0VLrA',\n",
    " 'IUsKp87a-v9Yhx6Ftg1m5A',\n",
    " 'iX-8Xm2G7meRHUg8qhoL1A',\n",
    " 'j5-4lzg23yGECBa6l1fyRQ',\n",
    " 'JG5s_bvRF1cSWf1fk9lTbw',\n",
    " 'JGpfPj8VEvnq1B-Xqr3w-A',\n",
    " 'JoQ5xekjQUkj8rukJIzqgg',\n",
    " 'jU-dKl2Ye4L_5x602yoctQ',\n",
    " 'juDNZOOnkgG3QINFrulsAg',\n",
    " 'JZZ716oX6_MqH6L_MkWK-A',\n",
    " 'K6pfRNwGodm1m1gFVQlj-Q',\n",
    " 'ke4ohxa93GJz0KH9H2kwsQ',\n",
    " 'kjMBhxBXOUE7SSUQb-YQbw',\n",
    " 'l2vR3PyVMF3pgIERdDEuiQ',\n",
    " 'LhLfsQtYwJ5OmEzilubhXQ',\n",
    " 'lrfy4UVIWtj0xwboLgUreQ',\n",
    " 'LXT4hCf1lRyUeM4HDBaSvg',\n",
    " 'l_rMdwgrvjm2PyHyXBcBTw',\n",
    " 'm3oIKhKKCQD54y1E-dBKSw',\n",
    " 'MduVueqYTBlEkX-axrh1ug',\n",
    " 'MZj64XNUN6Og178-6XYR6g',\n",
    " 'N6hL8FQ84A2DznF2S2Lp7g',\n",
    " 'n6Q9vNuxz7786ESEfautxQ',\n",
    " 'NfayhoTudVJQsEF-XlPyjw',\n",
    " 'NKEFWvRriK-LvagPz2QRxw',\n",
    " 'nKJ7yiPc0E_DJNtNxmCrhg',\n",
    " 'O0bVFyP58TOEix6IjERXQA',\n",
    " 'OK6HsALzFcBAUlrroKHZGg',\n",
    " 'PFD3ykdI1WVhvZ8IX4PmLQ',\n",
    " 'PjfJoBrEFgDrxiJy8nyatA',\n",
    " 'Pk87_8Yndygr4LRUD_H7Hg',\n",
    " 'pW1IPuTdLIUB61goirbXaA',\n",
    " 'pY32hIagdxrL4Nsi959EQg',\n",
    " 'QhATx1B1n8uf8C6siMNTfA',\n",
    " 'qMlGILrsrzhMDxajNYiyIA',\n",
    " 'QRUo4vqUu3X9V4eIqBpY8A',\n",
    " 'qxSXsYMA3aWuAfigeqeOOQ',\n",
    " 'RhC7TNmFvbR9GWrlrl5dsA',\n",
    " 'RIeulJUzgemFugkkgg4qgA',\n",
    " 'rIhUkEmP-j4NcQVW3YuPYQ',\n",
    " 'rLafN9k3_AF5lZU0cs3LZg',\n",
    " 'RLtBKD2rlfTaELWejmLBCA',\n",
    " 'rrfwGSwt3eHxxypfu5PGTA',\n",
    " 'tlp6LCLDsvL1GjO_kW_plQ',\n",
    " 'TN4-gAea6ejAdZ-NzYXxng',\n",
    " 'tSHz7RzlgceAItRejZ396A',\n",
    " 'TvD36_DdnyCJuXV1SSt3_Q',\n",
    " 't_sV6mI4oNvbvohhZAyeuA',\n",
    " 'UG2JuFFa_WxhPEtMOtq-JQ',\n",
    " 'VSekUmmsGZcX7KaPe_hXyw',\n",
    " 'w5ABnSadHC8z1lthMQBaBQ',\n",
    " 'W94rrCn0O5K1lkfD26m4tw',\n",
    " 'WGmGujPl5BmR_fCUZnoe9w',\n",
    " 'XX6ujA9CcB5s9y9wCy67-Q',\n",
    " 'Y3lA41pnMkQNGfyREkf6SA',\n",
    " 'yAf6R6OSgPo8-mmdDh8qIw',\n",
    " 'ydm3g1wUWSxJnMPgHk2JhQ',\n",
    " 'yFjqHyOaNFwzIWTV8EE9hg',\n",
    " 'yhztPWh5IhaePpUQJNW-dQ',\n",
    " 'ytJ4lihJrvyzMMRG-WwDNw',\n",
    " 'YW1WMOkVbdFBrixDnKgoqA',\n",
    " 'zTzdu2QqLozHpW_qYWF84w',\n",
    " '_exWW0g4Svg1Eo2YWsGzbg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "from tqdm import tqdm\n",
    "def load_business_rate(json_path):\n",
    "    \"\"\"\n",
    "    This function loads the image targets from a csv file. It assumes that the csv file\n",
    "    has a header row and that the first column contains the image path and all the subsequent\n",
    "    columns contain the target values which are bundled together into a numpy array.\n",
    "    \"\"\"\n",
    "    business_dict = {}\n",
    "    with open(json_path,'r',encoding='utf-8') as f:\n",
    "        f = ijson.parse(f,multiple_values = True)\n",
    "        for line in ijson.items(f,\"\"):\n",
    "            key = line['business_id']\n",
    "            value = line['stars']\n",
    "            business_dict.setdefault(key,[]).append(value)\n",
    "\n",
    "    for key in business_dict.keys():\n",
    "        value = business_dict.get(key,None)\n",
    "        business_dict[key] = [float(sum(value)/len(value))]      \n",
    "    return business_dict\n",
    "\n",
    "def load_image_rate(json_path, business_dict):\n",
    "    with open(json_path,'r',encoding='utf-8') as f:\n",
    "        f = ijson.parse(f,multiple_values = True)\n",
    "        for line in ijson.items(f,''):\n",
    "            if line['label'] == 'food':\n",
    "                if line['business_id'] in business_dict.keys() and line['photo_id'] not in remove:\n",
    "                    key = line['business_id']\n",
    "                    business_dict.get(key,None).append(line['photo_id']+'.jpg')\n",
    "    target_dict = {}\n",
    "    for k,v in business_dict.items():\n",
    "        rate = v.pop(0)\n",
    "        for item in v:\n",
    "            target_dict[item] = rate\n",
    "    business_dict.clear()\n",
    "    return target_dict\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RegressionImageFolder(torchvision.datasets.ImageFolder):\n",
    "    \"\"\"\n",
    "    The regression image folder is a subclass of the ImageFolder class and is designed for \n",
    "    image regression tasks rather than image classification tasks. It takes in a dictionary\n",
    "    that maps image paths to their target values.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, image_targets, *args, **kwargs):\n",
    "        super().__init__(root, *args, **kwargs)\n",
    "        paths, _ = zip(*self.imgs)\n",
    "        prefix = paths[0].split('\\\\')[0] + '\\\\'\n",
    "        \n",
    "        # filtered_list = [item for item in my_list if item in my_dict]\n",
    "        self.targets = list(image_targets.values())\n",
    "        paths = [ prefix + k for k in image_targets.keys()]\n",
    "        self.samples = self.imgs = list(zip(paths, self.targets))\n",
    "\n",
    "def make_data(train_tfm):\n",
    "    \"\"\"\n",
    "    Builds the train data loader\n",
    "    \"\"\"\n",
    "    business_dict = load_business_rate('yelp_academic_dataset_review.json')\n",
    "    targets = load_image_rate('photos.json', business_dict)\n",
    "    data = RegressionImageFolder(\n",
    "        'photo/', \n",
    "        image_targets= targets,\n",
    "        loader=lambda x: Image.open(x),\n",
    "        transform = train_tfm\n",
    "    )\n",
    "    # This constructs the dataloader that actually determins how images will be loaded in batches\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/vit-base-patch16-224\"\n",
    "processor = ViTImageProcessor.from_pretrained(model_name) \n",
    "\n",
    "mu, sigma = processor.image_mean, processor.image_std #get default mu,sigma\n",
    "size = processor.size\n",
    "\n",
    "norm = Normalize(mean=mu, std=sigma) #normalize image pixels range to [-1,1]\n",
    "\n",
    "# resize 3x32x32 to 3x224x224 -> convert to Pytorch tensor -> normalize\n",
    "transf = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    norm\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = make_data(transf)\n",
    "\n",
    "\n",
    "len_data = len(data_set)\n",
    "test_split = [int(len_data*0.8),len_data - int(len_data*0.8)]\n",
    "trainset_data, test_data = random_split(dataset=data_set, lengths=test_split,generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "len_train = len(trainset_data)\n",
    "val_split = [int(len_train*0.9),len_train - int(len_train*0.9)]\n",
    "train_data, val_data = random_split(dataset=trainset_data, lengths=val_split,generator=torch.Generator().manual_seed(0))\n",
    "input_dim = train_data[0][0].shape[0]\n",
    "\n",
    "def load_data(batch_size):\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader\n",
    "train_loader, val_loader, test_loader = load_data(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training - Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(model_name, num_labels=1, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"photo_to_rate\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=1e-2,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir='logs',\n",
    "    remove_unused_columns=False,\n",
    "    fp16 = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixels = torch.stack([example[0] for example in examples])\n",
    "    labels = torch.tensor([example[1] for example in examples])\n",
    "    return {\"pixel_values\": pixels, \"labels\": labels}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    return {'MAE': mean_absolute_error(predictions, labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75e9d1982bc407eac5b66c3912fc455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9369, 'grad_norm': 13.859210968017578, 'learning_rate': 1.9660668380462726e-05, 'epoch': 0.05}\n",
      "{'loss': 0.3678, 'grad_norm': 11.545600891113281, 'learning_rate': 1.9317909168808914e-05, 'epoch': 0.1}\n",
      "{'loss': 0.3442, 'grad_norm': 18.741853713989258, 'learning_rate': 1.89751499571551e-05, 'epoch': 0.15}\n",
      "{'loss': 0.3338, 'grad_norm': 9.862781524658203, 'learning_rate': 1.8632390745501287e-05, 'epoch': 0.21}\n",
      "{'loss': 0.339, 'grad_norm': 7.769491195678711, 'learning_rate': 1.8289631533847475e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3061, 'grad_norm': 3.020697832107544, 'learning_rate': 1.794687232219366e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3039, 'grad_norm': 8.16762924194336, 'learning_rate': 1.7604113110539848e-05, 'epoch': 0.36}\n",
      "{'loss': 0.2997, 'grad_norm': 7.172269344329834, 'learning_rate': 1.7261353898886033e-05, 'epoch': 0.41}\n",
      "{'loss': 0.2907, 'grad_norm': 11.108847618103027, 'learning_rate': 1.691859468723222e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3073, 'grad_norm': 20.016578674316406, 'learning_rate': 1.657583547557841e-05, 'epoch': 0.51}\n",
      "{'loss': 0.2968, 'grad_norm': 5.510396480560303, 'learning_rate': 1.6233076263924594e-05, 'epoch': 0.57}\n",
      "{'loss': 0.2933, 'grad_norm': 13.300806045532227, 'learning_rate': 1.589100257069409e-05, 'epoch': 0.62}\n",
      "{'loss': 0.3023, 'grad_norm': 16.264591217041016, 'learning_rate': 1.5548243359040277e-05, 'epoch': 0.67}\n",
      "{'loss': 0.2918, 'grad_norm': 6.7715840339660645, 'learning_rate': 1.5205484147386462e-05, 'epoch': 0.72}\n",
      "{'loss': 0.2876, 'grad_norm': 5.873107433319092, 'learning_rate': 1.4862724935732648e-05, 'epoch': 0.77}\n",
      "{'loss': 0.2983, 'grad_norm': 6.432795524597168, 'learning_rate': 1.4519965724078835e-05, 'epoch': 0.82}\n",
      "{'loss': 0.2819, 'grad_norm': 1.1999387741088867, 'learning_rate': 1.4177206512425023e-05, 'epoch': 0.87}\n",
      "{'loss': 0.2888, 'grad_norm': 28.552871704101562, 'learning_rate': 1.3835132819194517e-05, 'epoch': 0.93}\n",
      "{'loss': 0.2834, 'grad_norm': 10.100029945373535, 'learning_rate': 1.3492373607540703e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f94d848a8b0435f98eb61003734eb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2838090658187866, 'eval_MAE': 0.40679147839546204, 'eval_runtime': 101.5598, 'eval_samples_per_second': 85.112, 'eval_steps_per_second': 21.278, 'epoch': 1.0}\n",
      "{'loss': 0.2672, 'grad_norm': 13.932455062866211, 'learning_rate': 1.3149614395886891e-05, 'epoch': 1.03}\n",
      "{'loss': 0.2161, 'grad_norm': 3.863142490386963, 'learning_rate': 1.2806855184233077e-05, 'epoch': 1.08}\n",
      "{'loss': 0.2385, 'grad_norm': 6.299255847930908, 'learning_rate': 1.2464095972579264e-05, 'epoch': 1.13}\n",
      "{'loss': 0.2272, 'grad_norm': 9.900640487670898, 'learning_rate': 1.212133676092545e-05, 'epoch': 1.18}\n",
      "{'loss': 0.2298, 'grad_norm': 7.394041061401367, 'learning_rate': 1.1778577549271637e-05, 'epoch': 1.23}\n",
      "{'loss': 0.2396, 'grad_norm': 5.496857643127441, 'learning_rate': 1.1436503856041132e-05, 'epoch': 1.29}\n",
      "{'loss': 0.2307, 'grad_norm': 16.216854095458984, 'learning_rate': 1.1093744644387318e-05, 'epoch': 1.34}\n",
      "{'loss': 0.2268, 'grad_norm': 4.526978015899658, 'learning_rate': 1.0750985432733505e-05, 'epoch': 1.39}\n",
      "{'loss': 0.237, 'grad_norm': 7.612357139587402, 'learning_rate': 1.0408226221079691e-05, 'epoch': 1.44}\n",
      "{'loss': 0.2234, 'grad_norm': 12.261929512023926, 'learning_rate': 1.006546700942588e-05, 'epoch': 1.49}\n",
      "{'loss': 0.2255, 'grad_norm': 9.329120635986328, 'learning_rate': 9.722707797772066e-06, 'epoch': 1.54}\n",
      "{'loss': 0.2272, 'grad_norm': 6.290778636932373, 'learning_rate': 9.379948586118252e-06, 'epoch': 1.59}\n",
      "{'loss': 0.2368, 'grad_norm': 4.193188190460205, 'learning_rate': 9.037189374464439e-06, 'epoch': 1.65}\n",
      "{'loss': 0.2303, 'grad_norm': 6.411842346191406, 'learning_rate': 8.695115681233934e-06, 'epoch': 1.7}\n",
      "{'loss': 0.2282, 'grad_norm': 5.5075273513793945, 'learning_rate': 8.35235646958012e-06, 'epoch': 1.75}\n",
      "{'loss': 0.2204, 'grad_norm': 5.579601287841797, 'learning_rate': 8.009597257926307e-06, 'epoch': 1.8}\n",
      "{'loss': 0.2239, 'grad_norm': 4.946954250335693, 'learning_rate': 7.666838046272495e-06, 'epoch': 1.85}\n",
      "{'loss': 0.228, 'grad_norm': 4.840410232543945, 'learning_rate': 7.324764353041989e-06, 'epoch': 1.9}\n",
      "{'loss': 0.2279, 'grad_norm': 4.285760402679443, 'learning_rate': 6.982005141388175e-06, 'epoch': 1.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5577b210d91c4ab8ab6adb9def4b1b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2693431079387665, 'eval_MAE': 0.39130038022994995, 'eval_runtime': 78.9377, 'eval_samples_per_second': 109.504, 'eval_steps_per_second': 27.376, 'epoch': 2.0}\n",
      "{'loss': 0.2233, 'grad_norm': 4.629077434539795, 'learning_rate': 6.639245929734362e-06, 'epoch': 2.01}\n",
      "{'loss': 0.1606, 'grad_norm': 13.096073150634766, 'learning_rate': 6.296486718080549e-06, 'epoch': 2.06}\n",
      "{'loss': 0.1601, 'grad_norm': 5.3261003494262695, 'learning_rate': 5.954413024850043e-06, 'epoch': 2.11}\n",
      "{'loss': 0.1681, 'grad_norm': 7.779398441314697, 'learning_rate': 5.6116538131962305e-06, 'epoch': 2.16}\n",
      "{'loss': 0.1708, 'grad_norm': 7.1827311515808105, 'learning_rate': 5.268894601542417e-06, 'epoch': 2.21}\n",
      "{'loss': 0.1642, 'grad_norm': 13.48320484161377, 'learning_rate': 4.926135389888604e-06, 'epoch': 2.26}\n",
      "{'loss': 0.1577, 'grad_norm': 3.9348652362823486, 'learning_rate': 4.584061696658098e-06, 'epoch': 2.31}\n",
      "{'loss': 0.1595, 'grad_norm': 4.423639297485352, 'learning_rate': 4.241302485004285e-06, 'epoch': 2.37}\n",
      "{'loss': 0.1575, 'grad_norm': 12.875553131103516, 'learning_rate': 3.899228791773779e-06, 'epoch': 2.42}\n",
      "{'loss': 0.1507, 'grad_norm': 9.034147262573242, 'learning_rate': 3.556469580119966e-06, 'epoch': 2.47}\n",
      "{'loss': 0.1541, 'grad_norm': 10.575790405273438, 'learning_rate': 3.213710368466153e-06, 'epoch': 2.52}\n",
      "{'loss': 0.1531, 'grad_norm': 3.3329358100891113, 'learning_rate': 2.8709511568123393e-06, 'epoch': 2.57}\n",
      "{'loss': 0.1618, 'grad_norm': 4.552456855773926, 'learning_rate': 2.528191945158526e-06, 'epoch': 2.62}\n",
      "{'loss': 0.1488, 'grad_norm': 3.9277122020721436, 'learning_rate': 2.1854327335047134e-06, 'epoch': 2.67}\n",
      "{'loss': 0.1536, 'grad_norm': 6.675556659698486, 'learning_rate': 1.8426735218508998e-06, 'epoch': 2.72}\n",
      "{'loss': 0.1435, 'grad_norm': 4.168076038360596, 'learning_rate': 1.4999143101970866e-06, 'epoch': 2.78}\n",
      "{'loss': 0.1557, 'grad_norm': 7.143703937530518, 'learning_rate': 1.1578406169665812e-06, 'epoch': 2.83}\n",
      "{'loss': 0.1403, 'grad_norm': 4.899112224578857, 'learning_rate': 8.150814053127679e-07, 'epoch': 2.88}\n",
      "{'loss': 0.1493, 'grad_norm': 8.939935684204102, 'learning_rate': 4.723221936589546e-07, 'epoch': 2.93}\n",
      "{'loss': 0.1543, 'grad_norm': 6.8108696937561035, 'learning_rate': 1.2956298200514138e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9aac6ae6fbe4befa6d46b8da60609c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27013716101646423, 'eval_MAE': 0.3962051570415497, 'eval_runtime': 78.3988, 'eval_samples_per_second': 110.257, 'eval_steps_per_second': 27.564, 'epoch': 3.0}\n",
      "{'train_runtime': 4693.0671, 'train_samples_per_second': 49.728, 'train_steps_per_second': 6.217, 'train_loss': 0.24124454710763443, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=29175, training_loss=0.24124454710763443, metrics={'train_runtime': 4693.0671, 'train_samples_per_second': 49.728, 'train_steps_per_second': 6.217, 'train_loss': 0.24124454710763443, 'epoch': 3.0})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=processor)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args, \n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f68e4ce6b274127b7fa26e351084ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5403 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 0.26799851655960083, 'test_MAE': 0.3889068067073822, 'test_runtime': 747.8915, 'test_samples_per_second': 28.895, 'test_steps_per_second': 7.224}\n"
     ]
    }
   ],
   "source": [
    "outputs = trainer.predict(test_data)\n",
    "print(outputs.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 4. 4. ... 4. 4. 4.]\n",
      "[[    0   141    18     8     0]\n",
      " [    0   915   488   258     0]\n",
      " [    0   364  1298  2742    17]\n",
      " [    0    57   485 12342   178]\n",
      " [    0     1    22  2124   152]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "y_true = outputs.label_ids\n",
    "y_true = np.round(y_true)\n",
    "y_pred = outputs.predictions.squeeze()\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "data = {'y_true': y_true, 'y_pred': y_pred}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the dataframe as a CSV file\n",
    "df.to_csv('output2.csv', index=False)\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.84431138 0.10778443 0.04790419 0.        ]\n",
      " [0.         0.55087297 0.29379892 0.15532812 0.        ]\n",
      " [0.         0.08233431 0.29359873 0.62022167 0.00384528]\n",
      " [0.         0.0043638  0.03713061 0.94487827 0.01362732]\n",
      " [0.         0.00043497 0.00956938 0.92387995 0.0661157 ]]\n"
     ]
    }
   ],
   "source": [
    "def normalize_confusion_matrix(confusion_matrix):\n",
    "    # Compute the sum of each row\n",
    "    row_sum = np.sum(confusion_matrix, axis=1, keepdims=True)\n",
    "    # Divide each element in the confusion matrix by its respective row sum\n",
    "    normalized_matrix = confusion_matrix / row_sum\n",
    "    np.set_printoptions(suppress=True)\n",
    "    return normalized_matrix\n",
    "normalized_by_row = normalize_confusion_matrix(cm)\n",
    "print(normalized_by_row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "temp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
